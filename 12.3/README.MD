# Домашнее задание к занятию "12.3 Развертывание кластера на собственных серверах, лекция 1" 
1. Для корректной работы кластера требуется минимум один control plane. В тоже время, одна из важнейших его частей - это etcd, который желательно иметь более чем в одном экземпляре. Для достижения кворума вполне достаточно 3 нод с etcd, а прочие компоненты кластера можно разместить либо в том же количестве, либо на любых двух из них (api, control manager, scheduler). Таким образом нам потребуется **3 управляющие ноды - control plane** с оптимальными параметрами:
 - 4 ядра CPU
 - 8 Гб ОЗУ
 - 50Гб хранилища
 2. Согласно задания, нам требуется отказоустойчивость для СУБД, следовательно её копии должны быть представлены не просто подами в пределах одной ноды, а быть на различных нодах, чтобы в случае отказа ноды, поды с СУБД могли переехать на другую ноду. Кэш можно разместить на тех же нодах, что и СУБД. Таким образом на потребуется **4 рабочих ноды - кэш + СУБД**:
 - 8 ядер CPU
 - 16 Гб ОЗУ (из расчёта минимум 4 Гб для СУБД и предположением роста и остаток на кэш)
 - 200 Гб хранилища  
 Важно учесть, что в случае СУБД мы имеем дело с персистентными данными, а значит можно использовать и схему с каким-либо общим хранилищем либо на базе ceph или NFS. Тогда выпадение ноды СУБД не отразится на работоспособности.
 3. Для работы фронтенда и бекенда вполне возможно использовать одни и те же ноды. То есть в нашем случае нам могут потребоваться по минимуму **5 рабочих нод** на которых будет запускаться нужное количество экземпляров приложения:
 - 4 ядра CPU
 - 8Гб ОЗУ
 - 100Гб хранилища  
 Логика такая, что бэкенд будет задействован по 2 экземпляра на каждой из пяти нод, а фронтенд будет работать на 5 нодах. Количество ядер процессоров даст возможность без перегрузки работать разным элементам приложения. Дисковое хранилище выбрано исходя из того, что возможно будет некоторое количество логов от работы бекенда.
 4. Подробное описание:  
    4.1. Control plane состоит из 3-х нод с параметрами:  
    **3 node x 4 ядра CPU/8 Гб RAM/50 ГБ диск**  
    Все ноды содержат etcd, API server, controller manager, scheduler.  
    4.2. СУБД состоит из 4-х нод Master + 3 Slave (речь идёт о PostgreSQL) с параметрами:  
    **4 node x 8 ядер CPU/16 Гб RAM/200 Гб диск**  
    Одна нода содержит мастер и три другие слейв.  
    4.3. Кэш (например пусть это будет Redis) состоит из 3-х нод. Это те же самые ноды из п.4.2. Такая схема предусматривает по 2 копии Redis на каждой (Master/Slave) с перекрёстной работой (она часто упоминается в статьях).  
    4.4. Фронтенд и бекенд размещены на 5 нодах с параметрами:  
    **5 nodes x 4 ядер CPU/8 Гб RAM/100 Гб диск**  
    _Итого наш кластер состоит из 12 нод._

