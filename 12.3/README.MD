# Домашнее задание к занятию "12.3 Развертывание кластера на собственных серверах, лекция 1" 
1. Для корректной работы кластера требуется минимум один control plane. В тоже время, одна из важнейших его частей - это etcd, который желательно иметь более чем в одном экземпляре. Для достижения кворума вполне достаточно 3 нод с etcd, а прочие компоненты кластера можно разместить либо в том же количестве, либо на любых двух из них (api, control manager, scheduler). Таким образом нам потребуется **3 управляющие ноды - control plane** с оптимальными параметрами:
 - 4 ядра CPU
 - 8 Гб ОЗУ
 - 50Гб хранилища
 2. Согласно задания, нам требуется отказоустойчивость для СУБД, следовательно её копии должны быть представлены не просто подами в пределах одной ноды, а быть на различных нодах, чтобы в случае отказа ноды, поды с СУБД могли переехать на другую ноду. Кэш можно разместить на тех же нодах, что и СУБД. Таким образом на потребуется **4 рабочих ноды - кэш + СУБД**:
 - 8 ядер CPU
 - 16 Гб ОЗУ (из расчёта минимум 4 Гб для СУБД и предположением роста и остаток на кэш)
 - 200 Гб хранилища  
 Важно учесть, что в случае СУБД мы имеем дело с персистентными данными, а значит можно использовать и схему с каким-либо общим хранилищем либо на базе ceph или NFS. Тогда выпадение ноды СУБД не отразится на работоспособности.
 3. Для работы фронтенда и бекенда вполне возможно использовать одни и те же ноды. То есть в нашем случае нам могут потребоваться по минимуму **5 рабочих нод** на которых будет запускаться нужное количество экземпляров приложения:
 - 4 ядра CPU
 - 8Гб ОЗУ
 - 100Гб хранилища  
 Логика такая, что бэкенд будет задействован по 2 экземпляра на каждой из пяти нод, а фронтенд будет работать на 5 нодах. Количество ядер процессоров даст возможность без перегрузки работать разным элементам приложения. Дисковое хранилище выбрано исходя из того, что возможно будет некоторое количество логов от работы бекенда.